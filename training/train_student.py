"""
Training Pipeline Module
Trains a smaller student model using datasets generated by the RAG teacher.
"""
import json
import logging
import os
import sys
from pathlib import Path
from typing import Dict, List, Optional, Union

# Add project root to path to import config
sys.path.append(str(Path(__file__).parent.parent.absolute()))
import config

import torch
import transformers
from datasets import Dataset, load_dataset
from peft import (
    LoraConfig,
    TaskType,
    get_peft_model,
    prepare_model_for_kbit_training
)
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    Trainer,
    TrainingArguments
)

# Set up logging
logging.basicConfig(**config.LOGGING_CONFIG)
logger = logging.getLogger(__name__)


def load_training_dataset(
    dataset_path: str,
    format_type: str = "chat",
    tokenizer: Optional[transformers.PreTrainedTokenizer] = None,
    max_length: int = 2048,
    is_jsonl: bool = True
) -> Dataset:
    """
    Load and prepare the training dataset.
    
    Args:
        dataset_path: Path to dataset file or directory
        format_type: Dataset format type
        tokenizer: Tokenizer for formatting
        max_length: Maximum sequence length
        is_jsonl: Whether the dataset is in JSONL format
        
    Returns:
        Formatted Dataset for training
    """
    try:
        # Load dataset
        if is_jsonl:
            dataset = load_dataset('json', data_files=dataset_path, split='train')
        else:
            dataset = load_dataset(dataset_path, split='train')
            
        logger.info(f"Loaded dataset with {len(dataset)} examples from {dataset_path}")
        
        if tokenizer is None:
            logger.warning("No tokenizer provided, returning raw dataset")
            return dataset
            
        # Format based on type
        if format_type == "chat":
            return format_chat_dataset(dataset, tokenizer, max_length)
        elif format_type == "instruction":
            return format_instruction_dataset(dataset, tokenizer, max_length)
        elif format_type == "completion":
            return format_completion_dataset(dataset, tokenizer, max_length)
        else:
            logger.warning(f"Unknown format type: {format_type}, using chat format")
            return format_chat_dataset(dataset, tokenizer, max_length)
            
    except Exception as e:
        logger.error(f"Error loading dataset: {e}")
        raise


def format_chat_dataset(
    dataset: Dataset,
    tokenizer: transformers.PreTrainedTokenizer,
    max_length: int = 2048
) -> Dataset:
    """
    Format a dataset in chat format.
    
    Args:
        dataset: Raw dataset
        tokenizer: Tokenizer
        max_length: Maximum sequence length
        
    Returns:
        Formatted Dataset
    """
    def format_chat(example):
        conversation = []
        for message in example["messages"]:
            role = message["role"]
            content = message["content"]
            
            if role == "user":
                conversation.append(f"<s>[INST] {content} [/INST]")
            elif role == "assistant":
                conversation.append(f"{content}</s>")
                
        full_prompt = "".join(conversation)
        tokenized = tokenizer(full_prompt, truncation=True, max_length=max_length, padding="max_length")
        tokenized["labels"] = tokenized["input_ids"].copy()
        
        # Mask out the prompt tokens for loss calculation
        # Find the first assistant token position
        assistant_start = full_prompt.find("[/INST]") + len("[/INST]")
        assistant_tokens = tokenizer(full_prompt[assistant_start:], truncation=True, max_length=max_length)
        
        # Set user message tokens to -100 to ignore in loss calculation
        num_ignored = len(tokenized["input_ids"]) - len(assistant_tokens["input_ids"])
        tokenized["labels"] = [-100] * num_ignored + tokenized["labels"][num_ignored:]
        
        return tokenized
    
    return dataset.map(format_chat, remove_columns=dataset.column_names)


def format_instruction_dataset(
    dataset: Dataset,
    tokenizer: transformers.PreTrainedTokenizer,
    max_length: int = 2048
) -> Dataset:
    """
    Format a dataset in instruction format.
    
    Args:
        dataset: Raw dataset
        tokenizer: Tokenizer
        max_length: Maximum sequence length
        
    Returns:
        Formatted Dataset
    """
    def format_instruction(example):
        instruction = example["instruction"]
        input_text = example.get("input", "")
        output = example["output"]
        
        if input_text:
            prompt = f"<s>[INST] {instruction}\n\n{input_text} [/INST]"
        else:
            prompt = f"<s>[INST] {instruction} [/INST]"
            
        full_prompt = f"{prompt} {output}</s>"
        tokenized = tokenizer(full_prompt, truncation=True, max_length=max_length, padding="max_length")
        tokenized["labels"] = tokenized["input_ids"].copy()
        
        # Mask out the prompt tokens for loss calculation
        prompt_tokens = tokenizer(prompt, truncation=True, max_length=max_length)
        num_ignored = len(prompt_tokens["input_ids"])
        tokenized["labels"] = [-100] * num_ignored + tokenized["labels"][num_ignored:]
        
        return tokenized
    
    return dataset.map(format_instruction, remove_columns=dataset.column_names)


def format_completion_dataset(
    dataset: Dataset,
    tokenizer: transformers.PreTrainedTokenizer,
    max_length: int = 2048
) -> Dataset:
    """
    Format a dataset in completion format.
    
    Args:
        dataset: Raw dataset
        tokenizer: Tokenizer
        max_length: Maximum sequence length
        
    Returns:
        Formatted Dataset
    """
    def format_completion(example):
        prompt = example["prompt"]
        completion = example["completion"]
        
        full_prompt = f"{prompt}{completion}"
        tokenized = tokenizer(full_prompt, truncation=True, max_length=max_length, padding="max_length")
        tokenized["labels"] = tokenized["input_ids"].copy()
        
        # Mask out the prompt tokens for loss calculation
        prompt_tokens = tokenizer(prompt, truncation=True, max_length=max_length)
        num_ignored = len(prompt_tokens["input_ids"])
        tokenized["labels"] = [-100] * num_ignored + tokenized["labels"][num_ignored:]
        
        return tokenized
    
    return dataset.map(format_completion, remove_columns=dataset.column_names)


def initialize_model_and_tokenizer(
    model_name: str = config.STUDENT_MODEL_NAME,
    load_in_8bit: bool = False,
    load_in_4bit: bool = True,
    device_map: str = "auto"
) -> tuple:
    """
    Initialize model and tokenizer for fine-tuning.
    
    Args:
        model_name: Name or path of the model
        load_in_8bit: Whether to load in 8-bit quantization
        load_in_4bit: Whether to load in 4-bit quantization
        device_map: Device mapping strategy
        
    Returns:
        Tuple of model and tokenizer
    """
    try:
        logger.info(f"Loading model and tokenizer: {model_name}")
        
        # Configure quantization
        quantization_config = None
        if load_in_4bit:
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4"
            )
        elif load_in_8bit:
            quantization_config = BitsAndBytesConfig(
                load_in_8bit=True
            )
        
        # Load tokenizer
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            padding_side="right",
            trust_remote_code=True
        )
        
        # Ensure tokenizer has padding token
        if tokenizer.pad_token is None:
            if tokenizer.eos_token is not None:
                tokenizer.pad_token = tokenizer.eos_token
            else:
                tokenizer.pad_token = tokenizer.eos_token = "</s>"
        
        # Load model
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            device_map=device_map,
            trust_remote_code=True
        )
        
        # Prepare model for k-bit training if quantized
        if load_in_8bit or load_in_4bit:
            model = prepare_model_for_kbit_training(model)
        
        return model, tokenizer
        
    except Exception as e:
        logger.error(f"Error initializing model and tokenizer: {e}")
        raise


def setup_lora_for_model(
    model: transformers.PreTrainedModel,
    r: int = config.LORA_R,
    lora_alpha: int = config.LORA_ALPHA,
    lora_dropout: float = config.LORA_DROPOUT,
    bias: str = "none"
) -> transformers.PreTrainedModel:
    """
    Set up LoRA adapters for model fine-tuning.
    
    Args:
        model: Base model
        r: LoRA attention dimension
        lora_alpha: LoRA alpha
        lora_dropout: Dropout probability
        bias: Bias type
        
    Returns:
        Model with LoRA adapters
    """
    try:
        logger.info(f"Setting up LoRA with r={r}, alpha={lora_alpha}, dropout={lora_dropout}")
        
        # Configure LoRA
        target_modules = find_target_modules(model)
        
        # LoRA config
        peft_config = LoraConfig(
            r=r,
            lora_alpha=lora_alpha,
            lora_dropout=lora_dropout,
            bias=bias,
            task_type=TaskType.CAUSAL_LM,
            target_modules=target_modules
        )
        
        # Get PEFT model
        model = get_peft_model(model, peft_config)
        
        # Print trainable parameters info
        model.print_trainable_parameters()
        
        return model
        
    except Exception as e:
        logger.error(f"Error setting up LoRA: {e}")
        raise


def find_target_modules(model) -> List[str]:
    """
    Automatically find suitable target modules for LoRA.
    
    Args:
        model: Base model
        
    Returns:
        List of target module names
    """
    # Common target module names
    potential_targets = ["q_proj", "k_proj", "v_proj", "o_proj", "up_proj", "down_proj", 
                         "gate_proj", "query_key_value", "attention.output.dense"]
    
    # Find which ones exist in the model
    matching_targets = []
    
    # Get all attribute names
    for name, _ in model.named_modules():
        if any(target in name for target in potential_targets):
            parts = name.split('.')
            for part in parts:
                if part in potential_targets and part not in matching_targets:
                    matching_targets.append(part)
                    
    if not matching_targets:
        # Default fallback
        logger.warning("Could not automatically detect target modules, using default: ['query_key_value']")
        return ["query_key_value"]
    
    logger.info(f"Automatically detected target modules: {matching_targets}")
    return matching_targets


def train_model(
    model: transformers.PreTrainedModel,
    tokenizer: transformers.PreTrainedTokenizer,
    train_dataset: Dataset,
    val_dataset: Optional[Dataset] = None,
    output_dir: str = str(config.CHECKPOINTS_DIR),
    num_train_epochs: int = config.TRAINING_EPOCHS,
    per_device_train_batch_size: int = config.TRAINING_BATCH_SIZE,
    gradient_accumulation_steps: int = config.TRAINING_GRADIENT_ACCUMULATION_STEPS,
    learning_rate: float = config.TRAINING_LEARNING_RATE,
    weight_decay: float = 0.01,
    warmup_ratio: float = 0.03,
    lr_scheduler_type: str = "cosine",
    logging_steps: int = 10,
    save_steps: int = 100
) -> transformers.Trainer:
    """
    Train the model with LoRA fine-tuning.
    
    Args:
        model: Model with LoRA adapters
        tokenizer: Tokenizer
        train_dataset: Training dataset
        val_dataset: Validation dataset
        output_dir: Output directory
        num_train_epochs: Number of training epochs
        per_device_train_batch_size: Batch size per device
        gradient_accumulation_steps: Gradient accumulation steps
        learning_rate: Learning rate
        weight_decay: Weight decay
        warmup_ratio: Learning rate warmup ratio
        lr_scheduler_type: Learning rate scheduler type
        logging_steps: Logging frequency
        save_steps: Model saving frequency
        
    Returns:
        Trained Trainer object
    """
    try:
        # Setup output directory
        Path(output_dir).mkdir(exist_ok=True, parents=True)
        
        # Configure training arguments
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=num_train_epochs,
            per_device_train_batch_size=per_device_train_batch_size,
            gradient_accumulation_steps=gradient_accumulation_steps,
            learning_rate=learning_rate,
            weight_decay=weight_decay,
            warmup_ratio=warmup_ratio,
            lr_scheduler_type=lr_scheduler_type,
            logging_steps=logging_steps,
            save_strategy="steps",
            save_steps=save_steps,
            evaluation_strategy="steps" if val_dataset is not None else "no",
            eval_steps=save_steps if val_dataset is not None else None,
            load_best_model_at_end=val_dataset is not None,
            report_to=["tensorboard", "wandb"] if "WANDB_API_KEY" in os.environ else ["tensorboard"],
            disable_tqdm=False,
            remove_unused_columns=True,
            fp16=torch.cuda.is_available()
        )
        
        # Initialize Trainer
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            tokenizer=tokenizer
        )
        
        # Train model
        logger.info("Starting model training")
        trainer.train()
        
        # Save the final model
        trainer.save_model(os.path.join(output_dir, "final_model"))
        
        logger.info(f"Model training completed and saved to {output_dir}")
        
        return trainer
        
    except Exception as e:
        logger.error(f"Error training model: {e}")
        raise


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Train a student model using LoRA")
    parser.add_argument("--dataset", type=str, required=True, help="Path to training dataset file or directory")
    parser.add_argument("--val-dataset", type=str, help="Path to validation dataset (optional)")
    parser.add_argument("--model", type=str, default=config.STUDENT_MODEL_NAME, help="Model name or path")
    parser.add_argument("--output", type=str, default=str(config.CHECKPOINTS_DIR), help="Output directory")
    parser.add_argument("--format", type=str, choices=["chat", "instruction", "completion"], default="chat",
                        help="Dataset format")
    parser.add_argument("--epochs", type=int, default=config.TRAINING_EPOCHS, help="Number of training epochs")
    parser.add_argument("--batch-size", type=int, default=config.TRAINING_BATCH_SIZE, help="Batch size per device")
    parser.add_argument("--grad-accum", type=int, default=config.TRAINING_GRADIENT_ACCUMULATION_STEPS,
                        help="Gradient accumulation steps")
    parser.add_argument("--lr", type=float, default=config.TRAINING_LEARNING_RATE, help="Learning rate")
    parser.add_argument("--lora-r", type=int, default=config.LORA_R, help="LoRA attention dimension")
    parser.add_argument("--lora-alpha", type=int, default=config.LORA_ALPHA, help="LoRA alpha parameter")
    parser.add_argument("--lora-dropout", type=float, default=config.LORA_DROPOUT, help="LoRA dropout")
    parser.add_argument("--use-8bit", action="store_true", help="Use 8-bit quantization")
    parser.add_argument("--use-4bit", action="store_true", help="Use 4-bit quantization")
    
    args = parser.parse_args()
    
    try:
        # Initialize model and tokenizer
        model, tokenizer = initialize_model_and_tokenizer(
            args.model,
            load_in_8bit=args.use_8bit,
            load_in_4bit=args.use_4bit if not args.use_8bit else False
        )
        
        # Setup LoRA
        model = setup_lora_for_model(
            model,
            r=args.lora_r,
            lora_alpha=args.lora_alpha,
            lora_dropout=args.lora_dropout
        )
        
        # Load and format training dataset
        train_dataset = load_training_dataset(
            args.dataset,
            args.format,
            tokenizer
        )
        
        # Load and format validation dataset if provided
        val_dataset = None
        if args.val_dataset:
            val_dataset = load_training_dataset(
                args.val_dataset,
                args.format,
                tokenizer
            )
        
        # Train model
        trainer = train_model(
            model,
            tokenizer,
            train_dataset,
            val_dataset,
            args.output,
            args.epochs,
            args.batch_size,
            args.grad_accum,
            args.lr
        )
        
        print(f"Model training completed successfully!")
        
    except Exception as e:
        logger.error(f"Error in training pipeline: {e}")
        print(f"Error: {e}")
        sys.exit(1)
